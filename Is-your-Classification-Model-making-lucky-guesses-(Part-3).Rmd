---
title: "Is your Classification Model making lucky guesses (Part 3)"
author: "Shaheen"
date: "Monday, March 01, 2016"
output: html_document
---

### Is your Classification Model making lucky guesses (Part 3)  
Author: Shaheen Gauher

When we build a classification model, often we have to prove that our model is detecting the difference between different classes by learning from the data and not just making lucky guesses. How do we compare if our model performs better than a classifier built by assigning all instances to the majority label.  
We can build a majority label classifier by assigning the majority label to all the instances in the data. What would the accuracy of this classifier be compared to our machine learning model?
In the example below I will use iris data set to build a classification model using Decision Forest. Then I will compare the accuracy of this machine learning model to the accuracy of majority class classifier. The iris data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  

Note: The code below requires MRS (Microsoft R Server, formally Revolution R Enterprise (RRE))  
http://blog.revolutionanalytics.com/2016/01/microsoft-r-open.html    
MRS can be downloaded from https://www.dreamspark.com/Product/Product.aspx?productid=105



```{r}
#download data from
#https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
datad_iris=read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data", 
                        header=F,sep=",",na.strings="NA")  #150 rows and 5 columns for iris data
class(datad_iris)  #"data.frame"
#convert data frame to xdf object using rxDataStep() 
pathc=getwd()
iris_xdf =file.path(pathc,'iris_xdf.xdf') 
data_iris = rxDataStep(inData = datad_iris, outFile = iris_xdf ,
                  rowsPerRead=50, overwrite=TRUE)
class(data_iris)  # "RxXdfData"

#rename the label col 'V5' as 'LabelsCol'
names(data_iris)[names(data_iris)=='V5'] = 'LabelsCol'


#*** Split the data into training and testing data. ***

#use rxDataStep() to create a col called 'splitcol'to use for splitting
rxDataStep(inData=data_iris,outFile=data_iris,transforms=list(splitcol=factor(rbinom(.rxNumRows,1,0.8),labels=c('test','train'))),overwrite=T)  

#split using the col "splitcol" 
#rxSplit() -- Split an input '.xdf' file or data frame into multiple '.xdf' files or a list of data frames.
listofxdfs = rxSplit(data_iris,outFileBase='irissplit',outFileSuffixes=c("Train", "Test"),
                       splitByFactor = "splitcol",overwrite=T )

trainingdata = listofxdfs[[2]]
testdata = listofxdfs[[1]]

#Build Machine Learning model - Decision Forest rxDForest()

#collect names of columns (feature)to be used for modelling
allfeatures = setdiff(names(data_iris),c('LabelsCol','splitcol'))

#create formula for modelling
formula = as.formula(paste('LabelsCol',paste(allfeatures,collapse=' + '),sep=' ~ '))  
formula

#using rxDForest() to build ML model
DForest_model <- rxDForest(formula , data = trainingdata, seed = 10, cp = 0.01, nTree = 50, mTry = 2,                   
                    overwrite = TRUE, reportProgress = 0)
class(DForest_model) #"rxDForest" 
ML_model = DForest_model
ML_model

#Compute the accuracy of the model and compare to the accuracy of a Majority Class classifier.
train_modelout = rxPredict(ML_model, data = trainingdata, outData = 'temp.xdf', overwrite = TRUE, writeModelVars = TRUE, predVarNames = "PredictedLabels",type='class')
class(train_modelout) #"RxXdfData"


mytemp_xdf=RxXdfData("mytemp_xdf.xdf") #initialise an xdf object 
results_model = rxDataStep(inData = train_modelout, outFile = mytemp_xdf,varsToKeep = c('PredictedLabels','LabelsCol'),overwrite = TRUE )
dim(results_model)


cmatrix = rxCrossTabs(~PredictedLabels:LabelsCol,results_model,returnXtabs = T)
class(cmatrix) #"xtabs" "table"

#Compute accuracy of the model
accuracy = sum(diag(cmatrix)) / sum(cmatrix)
cat('accuracy=',accuracy,'\n')

#Accuracy of Majority Class classifier
num_eachclass = apply(cmatrix,1, sum)
majorityClass = which(num_eachclass==max(num_eachclass))
fraction_eachclass = num_eachclass / sum(cmatrix)

accuracy_maj_class = fraction_eachclass[majorityClass[1]]  
cat('accuracy_maj_class',accuracy_maj_class,'\n')

#remove file
if(file.exists("iris_xdf.xdf") ) {  file.remove("iris_xdf.xdf") }
```

